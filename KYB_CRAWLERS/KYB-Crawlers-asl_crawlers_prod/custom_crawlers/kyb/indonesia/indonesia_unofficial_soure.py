"""Import required library"""
import sys, traceback,time,re,json
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))
from helpers.load_env import ENV
from bs4 import BeautifulSoup
from CustomCrawler import CustomCrawler
import pandas as pd
from dateutil import parser

meta_data = {
    'SOURCE' : 'Indonesia Stock Exchange',
    'COUNTRY' : 'Indonesia',
    'CATEGORY' : 'Unofficial Source',
    'ENTITY_TYPE' : 'Company/Organization',
    'SOURCE_DETAIL' : {"Source URL": "https://idx.co.id/id/perusahaan-tercatat/profil-perusahaan-tercatat/", 
                        "Source Description": "Indonesia Stock Exchange (IDX) is a stock exchange based in Jakarta, Indonesia. It was previously known as the Jakarta Stock Exchange (JSX) before its name changed in 2007 after merging with the Surabaya Stock Exchange (SSX). During recent years, Indonesia Stock Exchange sees fastest membership growth in Asia. As of 2023, the Indonesia Stock Exchange had 833 listed companies, and total stock investors were about 6.4 million, compared to 2.5 million at the end of 2019."},
    'SOURCE_TYPE': 'HTML',
    'URL' : 'https://idx.co.id/id/perusahaan-tercatat/profil-perusahaan-tercatat/'
}

crawler_config = {
    'TRANSLATION_REQUIRED': False,
    'PROXY_REQUIRED': False,
    'CAPTCHA_REQUIRED': False,
    'CRAWLER_NAME': "Indonesia - Unofficial Source"
}
"""Global Variables"""
STATUS_CODE = 0
DATA_SIZE = 0
CONTENT_TYPE = 'N/A'

indonesia_crawler = CustomCrawler(meta_data=meta_data, config=crawler_config,ENV=ENV)
request_helper = indonesia_crawler.get_requests_helper()
selenium_helper = indonesia_crawler.get_selenium_helper()

def get_json_data(company_url):
    response = request_helper.make_request("https://proxy.webshare.io/api/v2/proxy/list/download/qtwdnlorrbofjrfyjjolbsiyvvkvcvocabovaehs/-/any/username/direct/-/")
    data = response.text
    lines = data.strip().split('\n')
    proxy_list = [line.replace('\r', '') for line in lines]

    for i in range(0, len(proxy_list)):
        try:
            # A driver is created for every request because the website detects and responds to traffic generated by automation tools.
            driver = selenium_helper.create_driver(headless=True, Nopecha=False, timeout=300, proxy=True, proxy_server=proxy_list[i])
            driver.get(company_url)
            time.sleep(3)
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            pre_element = soup.find('pre')
            json_str = pre_element.text
            json_data = json.loads(json_str)
            return json_data
        except:
            driver.quit()

def format_date(timestamp):
    date_str = ""
    try:
        datetime_obj = parser.parse(timestamp)
        date_str = datetime_obj.strftime("%d-%m-%Y")
    except Exception as e:
        pass
    return date_str

def get_data(data, company_name):
    item = {}
    addresses_detail = []
    additional_detail = []
    contacts_detail = []
    announcements_detail = []
    fillings_detail = []
    people_detail = []
    previous_names_detail = []
    if 'Profiles' in data and len(data['Profiles']) > 0:
        profile_data = data['Profiles'][0]
        item['name'] = profile_data.get('NamaEmiten', '')
        item['incorporation_date'] = format_date(profile_data.get('TanggalPencatatan', ''))
        item['code'] = profile_data.get('KodeEmiten', '')
        item['board_type'] = profile_data.get('PapanPencatatan', '')
        general_address = profile_data.get('Alamat','').replace("\r\n","").replace("\n\r","").strip()
        if general_address != '':
            addresses_detail.append({
                'type': 'general_address',
                'address': general_address
            })
        primary_sector = profile_data.get('KegiatanUsahaUtama')
        sector = profile_data.get('Sektor')
        subsector = profile_data.get('SubSektor')
        additional_detail.append({
            'type': 'sector_details',
            'data': [{
                'primary_sector': primary_sector,
                'sector': sector,
                'subsector': subsector
            }]
        })
        email_address = profile_data.get('Email')
        if email_address != '':
            contacts_detail.append({
                'type': 'email',
                'value': email_address
            }) 
        
        phone_number = profile_data.get('Telepon', '')
        if phone_number != '':
            contacts_detail.append({
                'type': 'phone_number',
                'value': phone_number
            }) 
        fax_number = profile_data.get('Fax', '')
        if fax_number != '':
            contacts_detail.append({
                'type': 'fax_number',
                'value': fax_number
            })
        item['industries'] = profile_data.get('Industri', '')
        item['sub_industry'] = profile_data.get('SubIndustri', '')
        item['tax_number'] = profile_data.get('NPWP', '')
        item['register'] = profile_data.get('BAE', '')

        website = profile_data.get('Website', '')
        if website != '':
            contacts_detail.append({
                'type': 'website',
                'value': website
            })
        dividend_info = []
        if 'Dividen' in data:
            for dividend in data['Dividen']:
                dividend_info.append({
                    'name': dividend.get('Nama', ''),
                    'year': dividend.get('TahunBuku', ''),
                    'cash_dividend': dividend.get('CashDividenTotal', ''),
                    'bonus': dividend.get('TotalSahamBonus', ''),
                    'cum_date': format_date(dividend.get('TanggalCum', '')),
                    'extended_date': format_date(dividend.get('TanggalExRegulerDanNegosiasi', '')),
                    'recording_date': format_date(dividend.get('TanggalDPS', '')),
                    'payment_date': format_date(dividend.get('TanggalPembayaran', ''))
                })
        if len(dividend_info) > 0:
            additional_detail.append({
                'type': 'dividend_info',
                'data': dividend_info
            })
        
        issued_history_info = []
        issued_history_json_data = get_json_data(f'https://idx.co.id/primary/ListingActivity/GetIssuedHistory?kodeEmiten={company_name}&start=0&length=9999')
        if 'data' in issued_history_json_data:
            for issued_list in issued_history_json_data['data']:
                issued_history_info.append({
                    "listing_date": format_date(issued_list.get('TanggalPencatatan', '')),
                    "action": issued_list.get('JenisTindakan', ''),
                    "stock_addition": issued_list.get('JumlahSaham', ''),
                    "stock_accumulation": issued_list.get('JumlahSahamSetelahTindakan', '')
                })
        if len(issued_history_info) > 0:
            additional_detail.append({
                'type': 'issued_history_info',
                'data': issued_history_info
            })

        announcements_json_data = get_json_data(f'https://idx.co.id/primary/ListedCompany/GetProfileAnnouncement?KodeEmiten={company_name}&indexFrom=0&pageSize=10&dateFrom=20221102&dateTo=20231103&lang=en&keyword=')
        if 'Replies' in announcements_json_data:
            for announcement in announcements_json_data['Replies']:
                announcements_detail.append({
                    'date': format_date(announcement['pengumuman']['TglPengumuman']) if 'pengumuman' in announcement and 'TglPengumuman' in announcement['pengumuman'] else '',
                    'title': announcement['pengumuman']['JudulPengumuman'] if 'pengumuman' in announcement and 'JudulPengumuman' in announcement['pengumuman'] else '',
                    'description': announcement['attachments'][0]['OriginalFilename'],
                    'meta_detail': {
                        'url' : f"https://idx.co.id{announcement['attachments'][0]['FullSavePath']}"
                    } if 'attachments' in announcement and len(announcement['attachments']) > 0 and 'FullSavePath' in announcement['attachments'][0] else {} 
                })

        fillings_json_data = get_json_data(f"https://idx.co.id/primary/ListedCompany/GetFinancialReport?periode=TW1&year=2023&indexFrom=0&pageSize=1000&reportType=rdf&kodeEmiten={company_name}")
        if 'Results' in fillings_json_data and len(fillings_json_data['Results']) > 0 and 'Attachments' in fillings_json_data:
            for fillings in fillings_json_data['Results'][0]['Attachments']:
                fillings_detail.append({
                    'title': fillings['File_Name'],
                    'url': f"https://idx.co.id{fillings['File_Path']}"
                })

        ownership_and_debt_info = []
        if 'BondsAndSukuk' in data:
            for bonds_and_sukuk in data['BondsAndSukuk']:
                ownership_and_debt_info.append({
                    'name': bonds_and_sukuk['NamaEmisi'],
                    'listing_date': format_date(bonds_and_sukuk['ListingDate']),
                    'mature_date': format_date(bonds_and_sukuk['MatureDate']),
                    'rating': bonds_and_sukuk['Rating'],
                    'value': bonds_and_sukuk['Nominal'],
                    'margin': bonds_and_sukuk['Margin'],
                    'trustee': bonds_and_sukuk['WaliAmanat'],
                    'isin_code': bonds_and_sukuk['ISINCode']
                })

        if len(ownership_and_debt_info) > 0:
            additional_detail.append({
                'type': 'ownership_and_debt_info',
                'data': ownership_and_debt_info
            })
        if 'Sekretaris' in data:
            for secretary in data['Sekretaris']:
                people_detail.append({
                    'designation': 'secretary',
                    'name': secretary['Nama'],
                    'email': secretary['Email'],
                    'phone_number': secretary['Telepon'],
                    'fax_number': secretary['Fax']
                })
        if 'Direktur' in data:
            for director in data['Direktur']:
                people_detail.append({
                    'name': director['Nama'],
                    'designation': director['Jabatan'],
                    'meta_detail': {
                        'affiliated':f"{director['Afiliasi']}"
                    }
                })

        if 'Komisaris' in data:
            for comissioners in data['Komisaris']:
                people_detail.append({
                    'name': comissioners['Nama'],
                    'designation': comissioners['Jabatan'],
                    'meta_detail': {
                        'independant': f"{comissioners['Independen']}"
                    }
                })
        if 'KomiteAudit' in data:
            for audit_committee in data['KomiteAudit']:
                people_detail.append({
                    'name': audit_committee['Nama'],
                    'designation': audit_committee['Jabatan'],
                })
        if 'PemegangSaham' in data:
            for shareholders in data['PemegangSaham']:
                people_detail.append({
                    'designation': 'shareholder',
                    'name': shareholders['Nama'],
                    'meta_detail': {
                        'type': shareholders['Kategori'],
                        'summary': shareholders['Jumlah'],
                        'percentage': shareholders['Persentase']
                    } 
                })

        subsidiary_info = []
        if 'AnakPerusahaan' in data:
            for subsidiary in data['AnakPerusahaan']:
                subsidiary_info.append({
                    'name': subsidiary['Nama'],
                    'type': subsidiary['BidangUsaha'],
                    'total_asset': subsidiary['JumlahAset'],
                    'percentage': subsidiary['Persentase']
                })
        if len(subsidiary_info) > 0:
            additional_detail.append({
                'type': 'subsidiary_info',
                'data': subsidiary_info
            })

        if 'KAP' in data:
            for public_accountant in data['KAP']:
                if 'name' in public_accountant and public_accountant['name'] != "":
                    previous_names_detail.append({
                        'name': public_accountant['name']
                    })

        item['addresses_detail'] = addresses_detail
        item['additional_detail'] = additional_detail
        item['contacts_detail'] = contacts_detail
        item['announcements_detail'] = announcements_detail
        item['fillings_detail'] = fillings_detail
        item['people_detail'] = people_detail
        item['previous_names_detail'] = previous_names_detail
    return item

try:
    start_url_num = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    df = pd.read_csv("input/company_urls.csv")
    for i, urls in enumerate(df.iterrows(), 1):
        if i < start_url_num:
            continue
        company_url = urls[1][0]
        print(f"Reocrd No: {i}")
        print("Company url:", company_url)
        url = f"https://idx.co.id/primary/ListedCompany/GetCompanyProfilesDetail?KodeEmiten={company_url.split('/')[-1]}&language=en-us"
        json_data = get_json_data(url)
        OBJ = get_data(json_data, company_url.split('/')[-1])
        OBJ =  indonesia_crawler.prepare_data_object(OBJ)
        OBJ['registration_number'] = ""
        ENTITY_ID = indonesia_crawler.generate_entity_id(company_name=OBJ['name'])
        NAME = OBJ['name']
        BIRTH_INCORPORATION_DATE = ""
        ROW = indonesia_crawler.prepare_row_for_db(ENTITY_ID,NAME,BIRTH_INCORPORATION_DATE,OBJ)
        indonesia_crawler.insert_record(ROW)
            
    log_data = {"status": 'success',
                    "error": "", "url": meta_data['URL'], "source_type": meta_data['SOURCE_TYPE'], "data_size": DATA_SIZE, "content_type": CONTENT_TYPE, "status_code": STATUS_CODE, "trace_back":"",  "crawler":"HTML"}
    
    indonesia_crawler.db_log(log_data)
    indonesia_crawler.end_crawler()
except Exception as e:
    tb = traceback.format_exc()
    print(e,tb)
    log_data = {"status": 'fail',
                    "error": e, "url": meta_data['URL'], "source_type": meta_data['SOURCE_TYPE'], "data_size": DATA_SIZE, "content_type": CONTENT_TYPE, "status_code": STATUS_CODE, "trace_back":tb.replace("'","''"),  "crawler":"HTML"}
    
    indonesia_crawler.db_log(log_data)
